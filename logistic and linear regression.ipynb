{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5c5893d",
   "metadata": {},
   "source": [
    "### 20221116作业：\n",
    "\n",
    "**袁靖松-3200105467**\n",
    "\n",
    "基于课堂展示的“机器学习选股范例”代码进行改造：\n",
    "\n",
    "作业可使用Jupyter Notebook完成，请大家独立完成。作业提交日期为2022年11月30日，请将作业分别交给所在组的助教。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bddbce01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T09:10:29.376059Z",
     "start_time": "2023-11-10T09:10:29.025459Z"
    }
   },
   "outputs": [],
   "source": [
    "#导入包\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70989fe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T02:22:26.771856Z",
     "start_time": "2023-11-08T02:22:26.758151Z"
    }
   },
   "outputs": [],
   "source": [
    "## 定义参数类\n",
    "#-- define a class including all parameters\n",
    "class Para():\n",
    "    \n",
    "    method = 'XGBOOST-C' # 'LOGI' 'XGBOOST-C' 'LR'  \n",
    "    \n",
    "    month_in_sample = range(82,153+1) #-- return 82~153 72 months\n",
    "    month_test = range(154,293+1) #-- return 154~293 140 months\n",
    "    \n",
    "    percent_select = [0.3,0.3] #-- 30% positive samples, 30% negative samples\n",
    "    percent_cv = 0.1 #-- percentage of cross validation samples 交叉验证的样本比例\n",
    "    \n",
    "    path_data = './data/csv_demo/'\n",
    "    path_results = './results/'\n",
    "    \n",
    "    seed = 42 #-- random seed\n",
    "    n_stock=5166\n",
    "    logi_c = 0.0006 #-- logistic regression parameter         #逻辑回归中正则化系数λ的倒数，越小的数值表示越强的正则化\n",
    "    \n",
    "    xgbc_n_estimators = 100 #-- xgboost classifier parameter  #迭代次数，即树的个数\n",
    "    xgbc_learning_rate = 0.1 #-- xgboost classifier parameter #通过减小每一步的权重，可以提高模型的Robust\n",
    "    xgbc_subsample_C = 0.95 #-- xgboost classifier parameter  #控制对于每棵树随机采样的比例。减小这个参数的值，可以避免过拟合。但如果设置得过小，可能会导致欠拟合。\n",
    "    xgbc_max_depth = 3 #-- xgboost classifier parameter       #树的最大深度，避免过拟合，max_depth越大，模型会学到更具体更局部的样本，需要使用CV函数来进行调优；\n",
    "\n",
    "para = Para()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa66d276",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T02:23:37.631393Z",
     "start_time": "2023-11-08T02:23:37.617197Z"
    }
   },
   "outputs": [],
   "source": [
    "## 生成二分类标签函数\n",
    "#-- function, label data \n",
    "def label_data(data):\n",
    "    #-- label data\n",
    "    data['return_bin'] = np.nan\n",
    "    \n",
    "    #-- sort by return\n",
    "    data = data.sort_values(by='return',ascending=False)\n",
    "    \n",
    "    #-- decide the amount of stocks selected\n",
    "    n_stock_select = np.multiply(para.percent_select,data.shape[0]) #计算矩阵的内积\n",
    "    n_stock_select = np.around(n_stock_select).astype(int) #结果为整数\n",
    "    \n",
    "    #-- assign 1 or 0\n",
    "    data.iloc[0:n_stock_select[0],-1] = 1  \n",
    "    data.iloc[-n_stock_select[1]:,-1] = 0\n",
    "    \n",
    "    #-- remove other stocks\n",
    "    data = data.dropna(axis=0)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a574749c",
   "metadata": {},
   "source": [
    "## 1. 原始代码为静态训练，可否改成滚动训练，如每次使用最近N个月数据训练，每半年更新一次模型？（提示：增加循环即可）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf0ffc0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T02:30:07.758726Z",
     "start_time": "2023-11-08T02:23:41.333277Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set, accuracy = 0.61\n",
      "training set, AUC = 0.65\n",
      "cv set, accuracy = 0.59\n",
      "cv set, AUC = 0.62\n",
      "testing set, month 155, accuracy = 0.59\n",
      "testing set, month 155, AUC = 0.64\n",
      "testing set, month 156, accuracy = 0.51\n",
      "testing set, month 156, AUC = 0.54\n",
      "testing set, month 157, accuracy = 0.55\n",
      "testing set, month 157, AUC = 0.59\n",
      "testing set, month 158, accuracy = 0.58\n",
      "testing set, month 158, AUC = 0.59\n",
      "testing set, month 159, accuracy = 0.54\n",
      "testing set, month 159, AUC = 0.58\n",
      "testing set, month 160, accuracy = 0.53\n",
      "testing set, month 160, AUC = 0.56\n",
      "training set, accuracy = 0.60\n",
      "training set, AUC = 0.64\n",
      "cv set, accuracy = 0.58\n",
      "cv set, AUC = 0.61\n",
      "testing set, month 161, accuracy = 0.58\n",
      "testing set, month 161, AUC = 0.63\n",
      "testing set, month 162, accuracy = 0.56\n",
      "testing set, month 162, AUC = 0.57\n",
      "testing set, month 163, accuracy = 0.56\n",
      "testing set, month 163, AUC = 0.59\n",
      "testing set, month 164, accuracy = 0.50\n",
      "testing set, month 164, AUC = 0.51\n",
      "testing set, month 165, accuracy = 0.62\n",
      "testing set, month 165, AUC = 0.67\n",
      "testing set, month 166, accuracy = 0.56\n",
      "testing set, month 166, AUC = 0.59\n",
      "training set, accuracy = 0.60\n",
      "training set, AUC = 0.64\n",
      "cv set, accuracy = 0.58\n",
      "cv set, AUC = 0.61\n",
      "testing set, month 167, accuracy = 0.62\n",
      "testing set, month 167, AUC = 0.66\n",
      "testing set, month 168, accuracy = 0.59\n",
      "testing set, month 168, AUC = 0.61\n",
      "testing set, month 169, accuracy = 0.58\n",
      "testing set, month 169, AUC = 0.61\n",
      "testing set, month 170, accuracy = 0.57\n",
      "testing set, month 170, AUC = 0.62\n",
      "testing set, month 171, accuracy = 0.53\n",
      "testing set, month 171, AUC = 0.54\n",
      "testing set, month 172, accuracy = 0.59\n",
      "testing set, month 172, AUC = 0.63\n",
      "training set, accuracy = 0.60\n",
      "training set, AUC = 0.64\n",
      "cv set, accuracy = 0.58\n",
      "cv set, AUC = 0.61\n",
      "testing set, month 173, accuracy = 0.56\n",
      "testing set, month 173, AUC = 0.59\n",
      "testing set, month 174, accuracy = 0.58\n",
      "testing set, month 174, AUC = 0.64\n",
      "testing set, month 175, accuracy = 0.55\n",
      "testing set, month 175, AUC = 0.56\n",
      "testing set, month 176, accuracy = 0.64\n",
      "testing set, month 176, AUC = 0.68\n",
      "testing set, month 177, accuracy = 0.60\n",
      "testing set, month 177, AUC = 0.64\n",
      "testing set, month 178, accuracy = 0.54\n",
      "testing set, month 178, AUC = 0.55\n",
      "training set, accuracy = 0.60\n",
      "training set, AUC = 0.64\n",
      "cv set, accuracy = 0.58\n",
      "cv set, AUC = 0.61\n",
      "testing set, month 179, accuracy = 0.58\n",
      "testing set, month 179, AUC = 0.63\n",
      "testing set, month 180, accuracy = 0.55\n",
      "testing set, month 180, AUC = 0.58\n",
      "testing set, month 181, accuracy = 0.54\n",
      "testing set, month 181, AUC = 0.57\n",
      "testing set, month 182, accuracy = 0.50\n",
      "testing set, month 182, AUC = 0.50\n",
      "testing set, month 183, accuracy = 0.52\n",
      "testing set, month 183, AUC = 0.56\n",
      "testing set, month 184, accuracy = 0.70\n",
      "testing set, month 184, AUC = 0.75\n",
      "training set, accuracy = 0.60\n",
      "training set, AUC = 0.63\n",
      "cv set, accuracy = 0.58\n",
      "cv set, AUC = 0.61\n",
      "testing set, month 185, accuracy = 0.53\n",
      "testing set, month 185, AUC = 0.55\n",
      "testing set, month 186, accuracy = 0.62\n",
      "testing set, month 186, AUC = 0.65\n",
      "testing set, month 187, accuracy = 0.56\n",
      "testing set, month 187, AUC = 0.57\n",
      "testing set, month 188, accuracy = 0.55\n",
      "testing set, month 188, AUC = 0.58\n",
      "testing set, month 189, accuracy = 0.51\n",
      "testing set, month 189, AUC = 0.50\n",
      "testing set, month 190, accuracy = 0.61\n",
      "testing set, month 190, AUC = 0.64\n",
      "training set, accuracy = 0.59\n",
      "training set, AUC = 0.63\n",
      "cv set, accuracy = 0.59\n",
      "cv set, AUC = 0.62\n",
      "testing set, month 191, accuracy = 0.64\n",
      "testing set, month 191, AUC = 0.70\n",
      "testing set, month 192, accuracy = 0.55\n",
      "testing set, month 192, AUC = 0.58\n",
      "testing set, month 193, accuracy = 0.58\n",
      "testing set, month 193, AUC = 0.61\n",
      "testing set, month 194, accuracy = 0.56\n",
      "testing set, month 194, AUC = 0.58\n",
      "testing set, month 195, accuracy = 0.60\n",
      "testing set, month 195, AUC = 0.67\n",
      "testing set, month 196, accuracy = 0.62\n",
      "testing set, month 196, AUC = 0.66\n",
      "training set, accuracy = 0.59\n",
      "training set, AUC = 0.63\n",
      "cv set, accuracy = 0.58\n",
      "cv set, AUC = 0.61\n",
      "testing set, month 197, accuracy = 0.60\n",
      "testing set, month 197, AUC = 0.66\n",
      "testing set, month 198, accuracy = 0.56\n",
      "testing set, month 198, AUC = 0.59\n",
      "testing set, month 199, accuracy = 0.50\n",
      "testing set, month 199, AUC = 0.49\n",
      "testing set, month 200, accuracy = 0.47\n",
      "testing set, month 200, AUC = 0.45\n",
      "testing set, month 201, accuracy = 0.67\n",
      "testing set, month 201, AUC = 0.72\n",
      "testing set, month 202, accuracy = 0.53\n",
      "testing set, month 202, AUC = 0.54\n",
      "training set, accuracy = 0.59\n",
      "training set, AUC = 0.63\n",
      "cv set, accuracy = 0.58\n",
      "cv set, AUC = 0.61\n",
      "testing set, month 203, accuracy = 0.57\n",
      "testing set, month 203, AUC = 0.61\n",
      "testing set, month 204, accuracy = 0.57\n",
      "testing set, month 204, AUC = 0.60\n",
      "testing set, month 205, accuracy = 0.57\n",
      "testing set, month 205, AUC = 0.60\n",
      "testing set, month 206, accuracy = 0.64\n",
      "testing set, month 206, AUC = 0.69\n",
      "testing set, month 207, accuracy = 0.59\n",
      "testing set, month 207, AUC = 0.62\n",
      "testing set, month 208, accuracy = 0.59\n",
      "testing set, month 208, AUC = 0.63\n",
      "training set, accuracy = 0.59\n",
      "training set, AUC = 0.63\n",
      "cv set, accuracy = 0.58\n",
      "cv set, AUC = 0.61\n",
      "testing set, month 209, accuracy = 0.66\n",
      "testing set, month 209, AUC = 0.71\n",
      "testing set, month 210, accuracy = 0.61\n",
      "testing set, month 210, AUC = 0.64\n",
      "testing set, month 211, accuracy = 0.60\n",
      "testing set, month 211, AUC = 0.65\n",
      "testing set, month 212, accuracy = 0.66\n",
      "testing set, month 212, AUC = 0.72\n",
      "testing set, month 213, accuracy = 0.59\n",
      "testing set, month 213, AUC = 0.63\n",
      "testing set, month 214, accuracy = 0.55\n",
      "testing set, month 214, AUC = 0.57\n",
      "training set, accuracy = 0.60\n",
      "training set, AUC = 0.64\n",
      "cv set, accuracy = 0.58\n",
      "cv set, AUC = 0.62\n",
      "testing set, month 215, accuracy = 0.67\n",
      "testing set, month 215, AUC = 0.72\n",
      "testing set, month 216, accuracy = 0.64\n",
      "testing set, month 216, AUC = 0.70\n",
      "testing set, month 217, accuracy = 0.53\n",
      "testing set, month 217, AUC = 0.54\n",
      "testing set, month 218, accuracy = 0.57\n",
      "testing set, month 218, AUC = 0.60\n",
      "testing set, month 219, accuracy = 0.64\n",
      "testing set, month 219, AUC = 0.70\n",
      "testing set, month 220, accuracy = 0.63\n",
      "testing set, month 220, AUC = 0.66\n",
      "training set, accuracy = 0.60\n",
      "training set, AUC = 0.64\n",
      "cv set, accuracy = 0.58\n",
      "cv set, AUC = 0.62\n",
      "testing set, month 221, accuracy = 0.64\n",
      "testing set, month 221, AUC = 0.68\n",
      "testing set, month 222, accuracy = 0.59\n",
      "testing set, month 222, AUC = 0.61\n",
      "testing set, month 223, accuracy = 0.56\n",
      "testing set, month 223, AUC = 0.60\n",
      "testing set, month 224, accuracy = 0.59\n",
      "testing set, month 224, AUC = 0.63\n",
      "testing set, month 225, accuracy = 0.46\n",
      "testing set, month 225, AUC = 0.47\n",
      "testing set, month 226, accuracy = 0.57\n",
      "testing set, month 226, AUC = 0.59\n",
      "training set, accuracy = 0.60\n",
      "training set, AUC = 0.64\n",
      "cv set, accuracy = 0.59\n",
      "cv set, AUC = 0.62\n",
      "testing set, month 227, accuracy = 0.51\n",
      "testing set, month 227, AUC = 0.50\n",
      "testing set, month 228, accuracy = 0.45\n",
      "testing set, month 228, AUC = 0.46\n",
      "testing set, month 229, accuracy = 0.44\n",
      "testing set, month 229, AUC = 0.43\n",
      "testing set, month 230, accuracy = 0.64\n",
      "testing set, month 230, AUC = 0.70\n",
      "testing set, month 231, accuracy = 0.52\n",
      "testing set, month 231, AUC = 0.53\n",
      "testing set, month 232, accuracy = 0.63\n",
      "testing set, month 232, AUC = 0.68\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 94\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m## 训练模型，交叉验证\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m#-- train model, and perform cross validation\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m#-- classification\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m para\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLOGI\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGBOOST-C\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m---> 94\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#投入训练数据，fit()函数\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m#-- y_pred: binary format; y_score: continious format\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     y_pred_train \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train)  \u001b[38;5;66;03m#预测输出结果，得到预测类别结果\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1516\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1488\u001b[0m (\n\u001b[0;32m   1489\u001b[0m     model,\n\u001b[0;32m   1490\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1495\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1496\u001b[0m )\n\u001b[0;32m   1497\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1498\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1499\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1513\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[0;32m   1514\u001b[0m )\n\u001b[1;32m-> 1516\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1519\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1528\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1531\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\xgboost\\training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1918\u001b[0m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1919\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1922\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#设置滚动训练时间\n",
    "train_data_min_months=72                      #每次模型训练所用数据最少不低于\n",
    "train_data_max_months=108                     #每次模型训练所用数据最大不超过\n",
    "train_update_months=6                         #设置更新周期\n",
    "start_date = 82                               #第一次滚动训练开始日期\n",
    "end_date = start_date + train_data_min_months #第一次滚动训练结束日期\n",
    "\n",
    "i=0\n",
    "\n",
    "## 样本外预测\n",
    "#-- predict\n",
    "\n",
    "d=pd.DataFrame([np.nan] * np.zeros((para.n_stock,end_date)))\n",
    "y_true_test=d\n",
    "y_pred_test=d\n",
    "y_score_test=d\n",
    "\n",
    "while end_date<= 293:\n",
    "    period_train=range(start_date,end_date+1)\n",
    "    \n",
    "    ## 生成样本内数据集\n",
    "    #-- generate in-sample data\n",
    "    for i_month in period_train:\n",
    "        #-- load csv\n",
    "        file_name = para.path_data + str(i_month) + '.csv'\n",
    "        data_curr_month = pd.read_csv(file_name, header = 0)#设置表头\n",
    "        para.n_stock = data_curr_month.shape[0]\n",
    "\n",
    "        #-- remove nan\n",
    "        data_curr_month = data_curr_month.dropna(axis=0)\n",
    "\n",
    "        #-- label data    \n",
    "        data_curr_month = label_data(data_curr_month) #调用函数，\n",
    "\n",
    "        #-- merge\n",
    "        if i_month == period_train[0]: #-- first month\n",
    "            data_in_sample = data_curr_month\n",
    "        else:\n",
    "            data_in_sample = pd.concat((data_in_sample,data_curr_month), axis=0)\n",
    "    \n",
    "    # 样本内数据集\n",
    "    #-- generate in-sample data\n",
    "    X_in_sample = data_in_sample.loc[:,'EP':'bias'] #提取数据\n",
    "\n",
    "    #-- classification\n",
    "    if para.method in ['LOGI','XGBOOST-C']:\n",
    "        y_in_sample = data_in_sample.loc[:,'return_bin']\n",
    "\n",
    "    #-- regression\n",
    "    if para.method in ['LR']:\n",
    "        y_in_sample = data_in_sample.loc[:,'return']\n",
    "        \n",
    "    ## 划分训练集和验证集\n",
    "    #-- generate train and cv data\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    #随机拆分数据为train set训练集和test set测试集\n",
    "    #X:要划分的样本特征集（输入的信息）\n",
    "    #y:需要划分的样本结果（输出结果）\n",
    "    #test_size:样本占比，测试集在总数中的百分比（小数表示）\n",
    "    #random_state:随机数种子，对于模型分割，必须用同一随机数种子，保证每次随机分割后数据集不变。\n",
    "\n",
    "    if para.percent_cv > 0:  \n",
    "        X_train, X_cv, y_train, y_cv = train_test_split(X_in_sample, y_in_sample, test_size=para.percent_cv, random_state=para.seed)\n",
    "    else:\n",
    "        X_train, y_train = X_in_sample.copy(), y_in_sample.copy()\n",
    "    \n",
    "    ## 设置模型\n",
    "    #-- set model\n",
    "    #有监督，输出是分类型数据，逻辑回归\n",
    "    #-- logistic regression\n",
    "    if para.method == 'LOGI':\n",
    "        from sklearn import linear_model\n",
    "        model = linear_model.LogisticRegression(C=para.logi_c)\n",
    "\n",
    "    #集成算法\n",
    "    #-- XGBoost Classifier 分类\n",
    "    if para.method == 'XGBOOST-C':\n",
    "        from xgboost import XGBClassifier\n",
    "        model = XGBClassifier(random_state=para.seed, \n",
    "                              n_estimators=para.xgbc_n_estimators,   #迭代次数\n",
    "                              learning_rate=para.xgbc_learning_rate, #减小每一步的权重，提高robust\n",
    "                              subsample=para.xgbc_subsample_C,       #随机采样比例\n",
    "                              max_depth=para.xgbc_max_depth)         #控制深度，避免过拟合\n",
    "    #有监督，连续型数据\n",
    "    #-- linear regression\n",
    "    if para.method == 'LR':\n",
    "        from sklearn import linear_model\n",
    "        model = linear_model.LinearRegression(fit_intercept=True)    #计算偏置 \n",
    "    ## 训练模型，交叉验证\n",
    "    #-- train model, and perform cross validation\n",
    "    #-- classification\n",
    "    if para.method in ['LOGI','XGBOOST-C']:\n",
    "        model.fit(X_train,y_train)  #投入训练数据，fit()函数\n",
    "        #-- y_pred: binary format; y_score: continious format\n",
    "        y_pred_train = model.predict(X_train)  #预测输出结果，得到预测类别结果\n",
    "        y_score_train = model.predict_proba(X_train)[:,1] #得到预测概率\n",
    "\n",
    "        if para.percent_cv > 0:  #0.1比例的验证集\n",
    "            y_pred_cv = model.predict(X_cv)\n",
    "            y_score_cv = model.predict_proba(X_cv)[:,1] \n",
    "            # 返回预测属于某标签的概率  \n",
    "\n",
    "    #-- regression\n",
    "    if para.method in ['LR']:\n",
    "        model.fit(X_train,y_train)\n",
    "        y_score_train = model.predict(X_train)\n",
    "\n",
    "        if para.percent_cv > 0:\n",
    "            y_score_cv = model.predict(X_cv)\n",
    "    \n",
    "    ##样本外预测集   \n",
    "    test_date_start = end_date + 1\n",
    "    test_date_end = end_date + 6\n",
    "    period_test=range(test_date_start,test_date_end+1)\n",
    "    \n",
    "    a=pd.DataFrame([np.nan] * np.zeros((para.n_stock,period_test[-1]+1)))\n",
    "    b=pd.DataFrame([np.nan] * np.zeros((para.n_stock,period_test[-1]+1)))\n",
    "    c=pd.DataFrame([np.nan] * np.zeros((para.n_stock,period_test[-1]+1)))\n",
    "    \n",
    "    for i_month in period_test:\n",
    "        #-- load\n",
    "        file_name = para.path_data + str(i_month) + '.csv'\n",
    "        data_curr_month = pd.read_csv(file_name, header = 0)\n",
    "        #-- remove nan  \n",
    "        data_curr_month = data_curr_month.dropna(axis=0)\n",
    "        #-- generate X\n",
    "        X_curr_month = data_curr_month.loc[:,'EP':'bias']\n",
    "        #-- pca\n",
    "        #X_curr_month = pca.transform(X_curr_month)\n",
    "        #-- predict and get predicted probability\n",
    "        #-- classification\n",
    "        if para.method in ['LOGI','XGBOOST-C']:\n",
    "            y_pred_curr_month = model.predict(X_curr_month)\n",
    "            y_score_curr_month = model.predict_proba(X_curr_month)[:,1]\n",
    "        #-- linear regression\n",
    "        if para.method in ['LR',]:\n",
    "            y_score_curr_month = model.predict(X_curr_month)\n",
    "        #-- store real and predicted return\n",
    "        a.iloc[data_curr_month.index,i_month-1] = data_curr_month['return'][data_curr_month.index]\n",
    "        if para.method in ['LOGI','XGBOOST-C']:\n",
    "            b.iloc[data_curr_month.index,i_month-1] = y_pred_curr_month\n",
    "        c.iloc[data_curr_month.index,i_month-1] = y_score_curr_month\n",
    "    #合并运行生成的数据\n",
    "    y_true_test=pd.concat([y_true_test,a.iloc[:,-7:-1]],axis=1)\n",
    "    y_pred_test=pd.concat([y_pred_test,b.iloc[:,-7:-1]],axis=1)\n",
    "    y_score_test=pd.concat([y_score_test,c.iloc[:,-7:-1]],axis=1)\n",
    "     \n",
    "     ## 模型评价\n",
    "    #-- evaluate\n",
    "    if para.method in ['LOGI','XGBOOST-C']:\n",
    "        from sklearn import metrics\n",
    "        print('training set, accuracy = %.2f'%metrics.accuracy_score(y_train, y_pred_train))\n",
    "        print('training set, AUC = %.2f'%metrics.roc_auc_score(y_train, y_score_train))\n",
    "        if para.percent_cv > 0:\n",
    "            print('cv set, accuracy = %.2f'%metrics.accuracy_score(y_cv, y_pred_cv))\n",
    "            print('cv set, AUC = %.2f'%metrics.roc_auc_score(y_cv, y_score_cv))\n",
    "        for i_month in period_test:\n",
    "            #-- 4 types of y\n",
    "            #-- y_true_*: true continious\n",
    "            #-- y_*: true binary\n",
    "            #-- y_pred_*: predicted binary\n",
    "            #-- y_score_*: predicted continious\n",
    "            y_true_curr_month = pd.DataFrame({'return':y_true_test.iloc[:,i_month-1]})\n",
    "            y_pred_curr_month = y_pred_test.iloc[:,i_month-1]\n",
    "            y_score_curr_month = y_score_test.iloc[:,i_month-1]\n",
    "            #-- remove nan\n",
    "            y_true_curr_month = y_true_curr_month.dropna(axis=0)\n",
    "            #-- label data\n",
    "            y_curr_month = label_data(y_true_curr_month)['return_bin']\n",
    "            #-- only select best and worst 30% data\n",
    "            y_pred_curr_month = y_pred_curr_month[y_curr_month.index]\n",
    "            y_score_curr_month = y_score_curr_month[y_curr_month.index]\n",
    "            print('testing set, month %d, accuracy = %.2f'%(i_month, metrics.accuracy_score(y_curr_month, y_pred_curr_month)))\n",
    "            print('testing set, month %d, AUC = %.2f'%(i_month, metrics.roc_auc_score(y_curr_month, y_score_curr_month)))\n",
    "    if para.method in ['LR']:\n",
    "        y_train.index = range(len(y_train))\n",
    "        y_score_train = pd.Series(y_score_train)\n",
    "        print('training set, ic = %.2f'%y_train.corr(y_score_train))\n",
    "        if para.percent_cv > 0:\n",
    "            y_cv.index = range(len(y_cv))\n",
    "            y_score_cv = pd.Series(y_score_cv)\n",
    "            print('cv set, ic = %.2f'%y_cv.corr(y_score_cv))\n",
    "        for i_month in period_test:\n",
    "            y_true_curr_month = y_true_test.iloc[:,i_month-1]\n",
    "            y_score_curr_month = y_score_test.iloc[:,i_month-1]\n",
    "            print('testing set, month %d, ic = %.2f'%(i_month, y_true_curr_month.corr(y_score_curr_month)))\n",
    "      \n",
    "    ##数据集滚动\n",
    "    end_date += train_update_months\n",
    "    if train_data_max_months <= end_date - start_date:\n",
    "        start_date = end_date - train_data_max_months\n",
    "    else:\n",
    "        start_date = start_date\n",
    "    if end_date+6>=293:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5036a811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annual excess return = 0.31\n",
      "annual excess volatility = 0.26\n",
      "information ratio = 1.17\n"
     ]
    }
   ],
   "source": [
    "## 简易回测\n",
    "#-- simple strategy, select 50 stocks every month, equally weighted\n",
    "para.n_stock_select = 50\n",
    "period_test=range(155,293)\n",
    "strategy = pd.DataFrame({'return':[0] * (period_test[-1]+1),'value':[1] * (period_test[-1]+1)})\n",
    "\n",
    "for i_month in period_test:\n",
    "    #-- get real and predicted return\n",
    "    y_true_curr_month = y_true_test.iloc[:,i_month-1]\n",
    "    y_score_curr_month = y_score_test.iloc[:,i_month-1]\n",
    "    \n",
    "    #-- sort predicted return, and choose the best 50\n",
    "    y_score_curr_month = y_score_curr_month.sort_values(ascending=False)\n",
    "    index_select = y_score_curr_month[0:para.n_stock_select].index\n",
    "    \n",
    "    #-- take the average return as the return of next month\n",
    "    strategy.loc[i_month-1,'return'] = np.mean(y_true_curr_month[index_select])\n",
    "\n",
    "    \n",
    "#-- compute the compund value of the strategy\n",
    "strategy['value'] = (strategy['return'] + 1).cumprod()\n",
    "\n",
    "#-- plot the value\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(155,292+1),strategy.loc[range(155,292+1),'value'],'r-')\n",
    "plt.show()\n",
    "\n",
    "#-- evaluation\n",
    "ann_excess_return = np.mean(strategy.loc[period_test,'return']) * 12\n",
    "ann_excess_vol = np.std(strategy.loc[period_test,'return']) * np.sqrt(12)\n",
    "info_ratio = ann_excess_return/ann_excess_vol\n",
    "\n",
    "print('annual excess return = %.2f'%ann_excess_return)\n",
    "print('annual excess volatility = %.2f'%ann_excess_vol)\n",
    "print('information ratio = %.2f'%info_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1054880c",
   "metadata": {},
   "source": [
    "## 2.原始代码未涉及交叉验证调参，验证集未实际发挥作用，可否添加调参模块？（提示：sklearn有现成工具）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cc56ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 生成样本内数据集\n",
    "#-- generate in-sample data\n",
    "for i_month in para.month_in_sample:\n",
    "    #-- load csv\n",
    "    file_name = para.path_data + str(i_month) + '.csv'\n",
    "    data_curr_month = pd.read_csv(file_name, header = 0)\n",
    "    para.n_stock = data_curr_month.shape[0]\n",
    "    \n",
    "    #-- remove nan\n",
    "    data_curr_month = data_curr_month.dropna(axis=0)\n",
    "    \n",
    "    #-- label data    \n",
    "    data_curr_month = label_data(data_curr_month)\n",
    "\n",
    "    #-- merge\n",
    "    if i_month == para.month_in_sample[0]: #-- first month\n",
    "        data_in_sample = data_curr_month\n",
    "    else:\n",
    "        data_in_sample = pd.concat((data_in_sample,data_curr_month), axis=0)\n",
    "# 样本内数据集\n",
    "#-- generate in-sample data\n",
    "X_in_sample = data_in_sample.loc[:,'EP':'bias']\n",
    "\n",
    "#-- classification\n",
    "if para.method in ['LOGI','XGBOOST-C']:\n",
    "    y_in_sample = data_in_sample.loc[:,'return_bin']\n",
    "\n",
    "#-- regression\n",
    "if para.method in ['LR']:\n",
    "    y_in_sample = data_in_sample.loc[:,'return']\n",
    "## 划分训练集和验证集\n",
    "#-- generate train and cv data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if para.percent_cv > 0:\n",
    "    X_train, X_cv, y_train, y_cv = train_test_split(X_in_sample, y_in_sample, test_size=para.percent_cv, random_state=para.seed)\n",
    "else:\n",
    "    X_train, y_train = X_in_sample.copy(), y_in_sample.copy()\n",
    "## 设置模型\n",
    "#-- set model\n",
    "\n",
    "#-- logistic regression\n",
    "if para.method == 'LOGI':\n",
    "    from sklearn import linear_model\n",
    "    model = linear_model.LogisticRegression(C=para.logi_c)\n",
    "\n",
    "#-- XGBoost Classifier\n",
    "if para.method == 'XGBOOST-C':\n",
    "    from xgboost import XGBClassifier\n",
    "    model = XGBClassifier(random_state=para.seed,\n",
    "                          n_estimators=para.xgbc_n_estimators,\n",
    "                          learning_rate=para.xgbc_learning_rate,\n",
    "                          subsample=para.xgbc_subsample_C,\n",
    "                          max_depth=para.xgbc_max_depth)\n",
    "\n",
    "#-- linear regression\n",
    "if para.method == 'LR':\n",
    "    from sklearn import linear_model\n",
    "    model = linear_model.LinearRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82bd4d08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.78815789, -0.6027193 , -0.61596491, -0.78075055, -0.78075055,\n",
      "       -0.80732892, -0.80732892, -0.78075055, -0.63456954, -0.75417219,\n",
      "       -0.88706402, -0.72774638, -0.67458495, -0.84735959, -0.82077888,\n",
      "       -0.63471388, -0.80748852, -0.78090781, -0.68787531, -0.84735959])]\n",
      "Best score is: -0.672137913793895\n",
      "Best parameter is: {'gamma': 0.01, 'learning_rate': 0.1, 'max_depth': 4}\n"
     ]
    }
   ],
   "source": [
    "## 训练模型，交叉验证\n",
    "#-- train model, and perform cross validation\n",
    "#-- classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "n_folds = 20 # 设置交叉检验的次数\n",
    "if para.method in ['LOGI','XGBOOST-C']:\n",
    "    cv_score_list = [] # 交叉检验结果列表\n",
    "    model.fit(X_train,y_train)\n",
    "    #-- y_pred: binary format; y_score: continious format\n",
    "    y_pred_train = model.predict(X_train) \n",
    "    y_score_train = model.predict_proba(X_train)[:,1]\n",
    "    \n",
    "    if para.percent_cv > 0:\n",
    "        #交叉验证\n",
    "        y_pred_cv = model.predict(X_cv)\n",
    "        y_score_cv = model.predict_proba(X_cv)[:,1]\n",
    "        scores = cross_val_score(model, X_cv, y_cv, cv = n_folds,scoring = 'r2')\n",
    "        cv_score_list.append(scores)\n",
    "        #调参\n",
    "        learning_rate = [0.01,0.1,0.2,0.3] #学习率\n",
    "        gamma = [1, 0.1, 0.01,]\n",
    "        max_depth=[2,3,4]\n",
    "\n",
    "        param_grid = dict(learning_rate = learning_rate,gamma = gamma,max_depth=max_depth)#转化为字典格式，网络搜索要求\n",
    "        model_gs = GridSearchCV(model, param_grid ,scoring = 'neg_log_loss',n_jobs = -1,cv=5)\n",
    "        model_gs.fit(X_train,y_train)\n",
    "        print(cv_score_list)\n",
    "        print('Best score is:',model_gs.best_score_) \n",
    "        print('Best parameter is:',model_gs.best_params_)   \n",
    "        \n",
    "#-- regression\n",
    "if para.method in ['LR']:\n",
    "    model.fit(X_train,y_train)\n",
    "    y_score_train = model.predict(X_train)\n",
    "    \n",
    "    if para.percent_cv > 0:\n",
    "        y_score_cv = model.predict(X_cv)\n",
    "        scores = cross_val_score(model, X_cv, y_cv, cv = n_folds,scoring = 'r2')\n",
    "        cv_score_list.append(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130e6beb",
   "metadata": {},
   "source": [
    "## 3.\t原始代码仅使用单一模型训练，其他机器学习方法表现如何，这些方法内部相关性如何？可否对低相关模型进行集成，进一步提升表现？（提示：绝大部分方法sklearn有现成工具；神经网络可使用tf、pytorch等编写，有一定难度）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50ceee23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression:\n",
      "mean squared error is: 0.42152358812568036\n",
      "mean absolute error is: 0.42152358812568036\n",
      "R Squared is: -0.6860953172981499\n",
      "RandomForest:\n",
      "mean squared error is: 0.0\n",
      "mean absolute error is: 0.0\n",
      "R Squared is: 1.0\n",
      "naive Bayes:\n",
      "mean squared error is: 0.44203981476356524\n",
      "mean absolute error is: 0.44203981476356524\n",
      "R Squared is: -0.7681602708078246\n",
      "xgboost:\n",
      "mean squared error is: 0.3684618364974816\n",
      "mean absolute error is: 0.3684618364974816\n",
      "R Squared is: -0.47384818933607686\n",
      "Accuracy: 0.58 (+/- 0.00) [Logistic Regression]\n",
      "Accuracy: 0.57 (+/- 0.00) [Random Forest]\n",
      "Accuracy: 0.56 (+/- 0.00) [naive Bayes]\n",
      "Accuracy: 0.58 (+/- 0.00) [xgboost]\n",
      "Accuracy: 0.58 (+/- 0.00) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "### Voting\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "clf1 = LogisticRegression(random_state=para.seed)\n",
    "clf1.fit(X_train,y_train)\n",
    "y_predict = clf1.predict(X_train)\n",
    "print('LogisticRegression:')\n",
    "print(f'mean squared error is: {mean_squared_error(y_train,y_predict)}')\n",
    "print(f'mean absolute error is: {mean_absolute_error(y_train,y_predict)}')\n",
    "print(f'R Squared is: {r2_score(y_train,y_predict)}')\n",
    "\n",
    "clf2 = RandomForestClassifier(n_estimators=para.xgbc_n_estimators, random_state=para.seed)\n",
    "clf2.fit(X_train,y_train)\n",
    "y_predict = clf2.predict(X_train)\n",
    "print('RandomForest:')\n",
    "print(f'mean squared error is: {mean_squared_error(y_train,y_predict)}')\n",
    "print(f'mean absolute error is: {mean_absolute_error(y_train,y_predict)}')\n",
    "print(f'R Squared is: {r2_score(y_train,y_predict)}')\n",
    "\n",
    "clf3 = GaussianNB()\n",
    "clf3.fit(X_train,y_train)\n",
    "y_predict = clf3.predict(X_train)\n",
    "print('naive Bayes:')\n",
    "print(f'mean squared error is: {mean_squared_error(y_train,y_predict)}')\n",
    "print(f'mean absolute error is: {mean_absolute_error(y_train,y_predict)}')\n",
    "print(f'R Squared is: {r2_score(y_train,y_predict)}')\n",
    "\n",
    "clf4 = XGBClassifier(random_state=para.seed,\n",
    "                   n_estimators=para.xgbc_n_estimators,\n",
    "                   learning_rate=para.xgbc_learning_rate,\n",
    "                   subsample=para.xgbc_subsample_C,\n",
    "                   max_depth=4)\n",
    "clf4.fit(X_train,y_train)\n",
    "y_predict = clf4.predict(X_train)\n",
    "print('xgboost:')\n",
    "print(f'mean squared error is: {mean_squared_error(y_train,y_predict)}')\n",
    "print(f'mean absolute error is: {mean_absolute_error(y_train,y_predict)}')\n",
    "print(f'R Squared is: {r2_score(y_train,y_predict)}')\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3),('xgb',clf4)],voting='hard')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3,clf4,eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes','xgboost', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv=5)\n",
    "    \n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb15bf0",
   "metadata": {},
   "source": [
    "## 4.\t原始策略在2017年后经历了较长的超额收益回撤（相对沪深300），可能是什么原因？有什么方法可以改进？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe5e4d7",
   "metadata": {},
   "source": [
    "#### 回撤：\n",
    "$EI为超额收益$\n",
    "\n",
    "$EI=\\frac{策略收益+100\\%}{基准收益+100\\%}$\n",
    "\n",
    "$ EI\\ Max\\ Drawdown=Max(EI_x-EI_y)/EI_x$\n",
    "\n",
    "#### 沪深300指数：\n",
    "沪深300指数由沪深市场中规模大、流动性好的最具代表性的300只证券组成，以反映沪深市场上市公司证券的整体表现；\n",
    "#### 2017年沪深300：\n",
    "- 2017年是蓝筹股行情，沪深300全年上涨；\n",
    "\n",
    "- 从所处行业、市值和业绩等因素来看，沪深300指数中的上市公司多数为绩优蓝筹股;\n",
    "\n",
    "- 可能与“去散户化”的行情有关，更多的积极投资者、机构投资者推动股市健康发展；\n",
    "\n",
    "- 结合当时背景，2015年股市大幅波动之后，投资者更加看重上市公司业绩，较多的绩差股则被投资者抛弃，且较多行业已呈现出寡头垄断的格局，行业龙头股也受到投资者青睐；\n",
    "\n",
    "- 此外，自2015年以来，监管层开始大力打击内幕交易等违法违规行为、炒作题材的市场风气，引导投资者做价值投资，也有利于蓝筹股行情的演绎;\n",
    "\n",
    "#### 策略不足：\n",
    "- 行业配置：行业风格切换会降低量化投资收益水平，低配与高配的行业配比未及时调整\n",
    "- 成交热度：2017年至2018年市场处于的低成交额水平，量化投资表现与市场成交额具有较强的正相关性\n",
    "- 量化投资技术发展迅速，但面临相同困境，出现大量同质化策略；\n",
    "- 市场风格切变：模型测试基于历史数据，市场未来充满较多的不确定性；\n",
    "\n",
    "#### 应对策略：\n",
    "- 关注在某些因子上的暴露程度，比如控制在行业或风格上的暴露，使行业轮动或风格切换不会太多影响Alpha水平。如果风格因子风险收益比降低，机构会减少暴露，控制风险；\n",
    "- 进行因子择时，根据市场环境调整因子权重，让模型与市场最新情况快速咬合；"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
